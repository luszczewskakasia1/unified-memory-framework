name: Compute Benchmarks

on:
  workflow_dispatch:
    inputs:
      pr_no:
        description: PR number (if 0, it'll run on the main)
        type: number
      bench_script_params:
        # If you want to save the results of the manual run in 'benchmark-results' branch,
        # you have to pass '--save XXX', where XXX is the label of your results.
        description: Parameters passed to script executing benchmark
        type: string
        required: false
        default: ''
      bench_script_compare:
        description: Compare label, passed to script executing benchmark as '--compare <label>'
        type: choice
        required: false
        default: 'Baseline_PVC'
        options:
          - 'Baseline_PVC'
      runner:
        description: Runner
        type: choice
        required: true
        default: 'L0_PERF'
        options:
          - L0_PERF

permissions:
  contents: read

jobs:
  manual:
    permissions:
      contents: write
      pull-requests: write
    name: Compute Benchmarks
    uses: ./.github/workflows/reusable_benchmarks.yml
    with:
      pr_no: ${{ inputs.pr_no }}
      bench_script_params: ${{ inputs.bench_script_params }}
      bench_script_compare: ${{ inputs.bench_script_compare }}
      runner: ${{ inputs.runner }}
